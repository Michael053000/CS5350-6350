import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# Shared Data Loading and Preprocessing
train_data = pd.read_csv("train.csv", header=None)
test_data = pd.read_csv("test.csv", header=None)

X = train_data.iloc[:, :-1].values
y = train_data.iloc[:, -1].values

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

epochs = 100
gamma_0 = 0.5
d = 0.001
widths = [5, 10, 25, 50, 100]

# --- S gradient ---
def gradient_descent(X_train, y_train, hidden_size, gamma_0, d, epochs):
    np.random.seed(42)
    input_size = X_train.shape[1]
    output_size = 1

    weights = {
        'hidden': np.random.randn(input_size, hidden_size),
        'output': np.random.randn(hidden_size, output_size)
    }

    losses = []
    for epoch in range(epochs):
        total_loss = 0
        for i in range(len(X_train)):
            X = X_train[i]
            y = y_train[i]

            gamma_t = gamma_0 / (1 + (gamma_0 / d) * (epoch * len(X_train) + i))

            hidden_output = 1 / (1 + np.exp(-np.dot(X, weights['hidden'])))
            output = 1 / (1 + np.exp(-np.dot(hidden_output, weights['output'])))

            error = y - output
            total_loss += error**2

            output_delta = error * output * (1 - output)
            hidden_delta = hidden_output * (1 - hidden_output) * np.dot(weights['output'], output_delta)

            weights['output'] += gamma_t * np.outer(hidden_output, output_delta)
            weights['hidden'] += gamma_t * np.outer(X, hidden_delta)

        losses.append(total_loss / len(X_train))

    return weights, losses

sgradient_results = []
for hidden_size in widths:
    weights, losses = gradient_descent(X_train, y_train, hidden_size, gamma_0, d, epochs)
    sgradient_results.append({'hidden_size': hidden_size, 'weights': weights, 'losses': losses})

# --- Plot Sgradient Loss ---
for result in sgradient_results:
    plt.plot(result['losses'], label=f"Hidden {result['hidden_size']}")
plt.title("Sgradient Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# --- S gradient Zero.py ---
def gradient_zero_initialization(X_train, y_train, hidden_size, gamma_0, d, epochs):
    input_size = X_train.shape[1]
    output_size = 1

    weights = {
        'hidden': np.zeros((input_size, hidden_size)),
        'output': np.zeros((hidden_size, output_size))
    }

    losses = []
    for epoch in range(epochs):
        total_loss = 0
        for i in range(len(X_train)):
            X = X_train[i]
            y = y_train[i]

            gamma_t = gamma_0 / (1 + (gamma_0 / d) * (epoch * len(X_train) + i))

            hidden_output = 1 / (1 + np.exp(-np.dot(X, weights['hidden'])))
            output = 1 / (1 + np.exp(-np.dot(hidden_output, weights['output'])))

            error = y - output
            total_loss += error**2

            output_delta = error * output * (1 - output)
            hidden_delta = hidden_output * (1 - hidden_output) * np.dot(weights['output'], output_delta)

            weights['output'] += gamma_t * np.outer(hidden_output, output_delta)
            weights['hidden'] += gamma_t * np.outer(X, hidden_delta)

        losses.append(total_loss / len(X_train))

    return weights, losses

gradient_zero_results = []
for hidden_size in widths:
    weights, losses = gradient_zero_initialization(X_train, y_train, hidden_size, gamma_0, d, epochs)
    gradient_zero_results.append({'hidden_size': hidden_size, 'weights': weights, 'losses': losses})

# --- Plot SgradientZero Loss ---
for result in gradient_zero_results:
    plt.plot(result['losses'], label=f"Hidden {result['hidden_size']}")
plt.title("SgradientZero Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# --- back propagation ---
def backpropagation(X_train, y_train, hidden_size, gamma_0, d, epochs):
    input_size = X_train.shape[1]
    output_size = 1
    np.random.seed(42)

    weights = {
        'hidden': np.random.randn(input_size, hidden_size),
        'output': np.random.randn(hidden_size, output_size)
    }

    losses = []
    for epoch in range(epochs):
        total_loss = 0
        for i in range(len(X_train)):
            X = X_train[i]
            y = y_train[i]

            gamma_t = gamma_0 / (1 + (gamma_0 / d) * (epoch * len(X_train) + i))

            hidden_input = np.dot(X, weights['hidden'])
            hidden_output = 1 / (1 + np.exp(-hidden_input))

            output_input = np.dot(hidden_output, weights['output'])
            output = 1 / (1 + np.exp(-output_input))

            error = y - output
            total_loss += error**2

            output_delta = error * output * (1 - output)

            hidden_error = np.dot(weights['output'], output_delta)
            hidden_delta = hidden_error * hidden_output * (1 - hidden_output)

            weights['output'] += gamma_t * np.outer(hidden_output, output_delta)
            weights['hidden'] += gamma_t * np.outer(X, hidden_delta)

        losses.append(total_loss / len(X_train))

    return weights, losses

backprop_results = []
for hidden_size in widths:
    weights, losses = backpropagation(X_train, y_train, hidden_size, gamma_0, d, epochs)
    backprop_results.append({'hidden_size': hidden_size, 'weights': weights, 'losses': losses})

# --- Plot Backpropagation Loss ---
for result in backprop_results:
    plt.plot(result['losses'], label=f"Hidden {result['hidden_size']}")
plt.title("Backpropagation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# --- pytorch ---
def pytorch_model(X_train, y_train, hidden_size, epochs):
    class NeuralNet(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(NeuralNet, self).__init__()
            self.hidden = nn.Linear(input_size, hidden_size)
            self.output = nn.Linear(hidden_size, output_size)

        def forward(self, x):
            x = torch.sigmoid(self.hidden(x))
            x = torch.sigmoid(self.output(x))
            return x

    input_size = X_train.shape[1]
    output_size = 1
    model = NeuralNet(input_size, hidden_size, output_size)
    optimizer = torch.optim.SGD(model.parameters(), lr=gamma_0)
    criterion = nn.MSELoss()

    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)

    losses = []
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

    return model, losses

pytorch_results = []
for hidden_size in widths:
    model, losses = pytorch_model(X_train, y_train, hidden_size, epochs)
    pytorch_results.append({'hidden_size': hidden_size, 'model': model, 'losses': losses})

# --- Plot PyTorch Loss ---
for result in pytorch_results:
    plt.plot(result['losses'], label=f"Hidden {result['hidden_size']}")
plt.title("PyTorch Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Print Results
print("\n--- S gradient Results ---")
for result in sgradient_results:
    print(f"Hidden Size: {result['hidden_size']}, Final Loss: {result['losses'][-1]}")

print("\n--- S gradient Zero Results ---")
for result in gradient_zero_results:
    print(f"Hidden Size: {result['hidden_size']}, Final Loss: {result['losses'][-1]}")

print("\n--- Back Propagation Results ---")
for result in backprop_results:
    print(f"Hidden Size: {result['hidden_size']}, Final Loss: {result['losses'][-1]}")

print("\n--- PyTorch Results ---")
for result in pytorch_results:
    print(f"Hidden Size: {result['hidden_size']}, Final Loss: {result['losses'][-1]}")
